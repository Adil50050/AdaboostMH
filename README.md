## 样本
选择的是http://archive.ics.uci.edu/ml 上的letter数据集

## 存在的问题
主要是对sklearn不太熟悉，所以存在一些问题可以优化

1. 弱分类器选择
现在选择的是GaussianNB，所有参数需要是连续型，因此我把扩展的类名转为对应的ascii值，可能会有问题

2. 最终结果
原来的论文里没讲，我在网上也没找到对应的处理方法。  
现在的做法是，对于一个测试样本，按照训练时的做法，将测试数据扩展为二分类问题，最终错误率为二分类问题的错误率，但是没有给出多类问题对应的类


## Version 2: preprocess.py + adamh2.py
在第一个版本的基础上，我使用了所有sklearn提供的分类器，都不行。（如DecisionTree，第一个弱分类器准确率就已到达100%，naive_bayes弱分类器结果均为0,不具备分类能力。）所以我又写了一遍，防止是什么地方写错了。。

## V3. preprocess.py + adamh2.py
使用sklearn自带的AdaBoost
也不可以。运行之后会发现，对于扩展之后的数据集，adaboost分类器的结果和之前naive_bayes分类器结果一样，全部都是0

我觉得造成这种结果的原因：
1. 扩展后的数据集，正反例比例差得太多，对adaboost MH影响可能会比较大
2. 这个数据集有毒。。

###后续建议
更换数据集试试。
v2, v3的代码都没有问题，简单修改就可以用倒其他数据集上。
我一不小心通宵了。。白天要废了，剩下的交给你们了,,,,,
